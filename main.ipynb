{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwuBFVXevzyy"
      },
      "source": [
        "# DDQN Lunar Lander\n",
        "Implementation of Double Deep Q-Network (DDQN) to solve the game Lunar Lander by earning more than +200 total reward on average over 100 trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBHaS7hCwfAD"
      },
      "outputs": [],
      "source": [
        "#if you want to run code on colab\n",
        "#dependencies\n",
        "!pip install gym==0.15.3\n",
        "!pip install gym[box2d]\n",
        "\n",
        "\n",
        "#render on colab, TODO: complete it later: https://stackoverflow.com/questions/50107530/how-to-render-openai-gym-in-google-colab\n",
        "!apt-get install x11-utils > /dev/null 2>&1 \n",
        "!pip install pyglet > /dev/null 2>&1 \n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "def env_render_colab(screen):\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import tile coding software\n",
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd /content/gdrive/My Drive\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive')\n",
        "import tiles3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FZnzqcaLqST",
        "outputId": "ae31af00-3e59-4a9a-97a3-9349335cb59d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0RfST9nPvzy6"
      },
      "outputs": [],
      "source": [
        "# import dependency\n",
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IC-ZbRjpvzy7"
      },
      "outputs": [],
      "source": [
        "# path\n",
        "CURR_PATH = os.path.abspath('')\n",
        "OUTPUT_PATH = os.path.join(CURR_PATH, 'output')\n",
        "RANDOM_AGENT_PATH = os.path.join(OUTPUT_PATH, 'random_agent')\n",
        "DDQN_AGENT_PATH = os.path.join(OUTPUT_PATH, 'ddqn_agent')\n",
        "DDQN_CHECKPOINT_PATH = os.path.join(DDQN_AGENT_PATH, 'policy_model_checkpoint.pth')\n",
        "DDQN_RESULT_IMG_PATH = os.path.join(DDQN_AGENT_PATH, 'result_img_{}.png')\n",
        "\n",
        "for p in [RANDOM_AGENT_PATH, DDQN_AGENT_PATH]:\n",
        "    if not os.path.exists(p): \n",
        "        os.makedirs(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCo4W16Dvzy8"
      },
      "source": [
        "## Lunar Lander"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV4wiLZGvzy9"
      },
      "source": [
        "![Lunar Lander](res/Lunar_Lander.gif)\n",
        "<center>(https://gym.openai.com/envs/LunarLander-v2/)</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLTiDSAxvzy-"
      },
      "source": [
        "Lunar Lander of OpenAI Gym is the game we want to solve in this work (Brockman et al. 2016). It is an interactive environment for an agent to land a rocket on a planet. A state here can be represented by an 8-dimensional continuous space:\n",
        "$$ (x, y, v_{x}, v_{y}, v_{\\theta}, leg_{left}, leg_{right}) $$\n",
        ", where $x$ and $y$ are the coordinates of the lander's position; $v_{x}$ and $v_{y}$ are the velocity components on two axes; $\\theta$ and $v_{\\theta}$ are the angle and the angular velocity separately; $leg_{left}$ and $leg_{right}$ are two binary values standing for whether the left or right leg of the lander is touching the ground.\n",
        "\n",
        "For each time step, there are four discrete actions available, that is, doing nothing, firing the left orientation engine, firing the main engine, and firing the right orientation engine.\n",
        "\n",
        "The game will be over or passed if the lander crashes or comes to rest. The reward for a bad ending is -100, while that for a happy ending is +100. The touching of the leg to the ground can generate a +10 reward, but each time of firing the main engine incurs a -0.3 penalty. Therefore, the total reward for a single episode ranges from 100 to more than 200 based on the final location of the lander on the pad. The distance between the landing pad and the lander will cause a penalization, which equals to the reward gained by moving closer to the pad. \n",
        "\n",
        "A method will be treated as a successful solution for this game if it can achieve more than +200 points average over 100 consecutive episodes.\n",
        "\n",
        "Note:\n",
        "1. six dimensional continuous state space with two more discrete variables\n",
        "2. discrete action space\n",
        "3. landing pad at coordinates $(0, 0)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N5jdfENrvzzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6675ad8-c524-42c5-f414-60d279bdb4a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state space: 8\n",
            "action space: 4\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "n_states, n_actions = env.observation_space.shape[0], env.action_space.n\n",
        "print('state space: {}'.format(n_states))\n",
        "print('action space: {}'.format(n_actions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff1dYEp8vzzD"
      },
      "source": [
        "## Random Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "H6jaU8vWvzzE"
      },
      "outputs": [],
      "source": [
        "# take a look at how a random agent act in the game\n",
        "# On OS X, you can install ffmpeg via `brew install ffmpeg`. \n",
        "# On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. \n",
        "# On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\n",
        "\n",
        "class RandomAgent(object):\n",
        "    \"\"\"The world's simplest agent!\"\"\"\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def act(self):\n",
        "        return self.action_space.sample()\n",
        "\n",
        "env = wrappers.Monitor(env, RANDOM_AGENT_PATH, force=True)\n",
        "agent = RandomAgent(env.action_space)\n",
        "ob = env.reset() #reinitialize environment and return initial state\n",
        "while True:\n",
        "    action = agent.act()\n",
        "    _, _, done,_ = env.step(action) #observation(St+1),rewar(Rt+1),done(is_terminal),info(general information for debugging based on environment, e.g. number of lives left)\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "\n",
        "# find video record under ./output/random_agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "874Psg7vvzzF"
      },
      "source": [
        "## Double Deep Q-Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzJ-Z3DHvzzG"
      },
      "source": [
        "### Model Graph, Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WwrStQoHvzzH"
      },
      "outputs": [],
      "source": [
        "class DDQN_Graph(nn.Module): \n",
        "    \"\"\"\n",
        "    Deep Reinforcement Learning with Double Q-Learning by Hasselt et al. (2016)\n",
        "    Double Deep Q-Network Model Graph\n",
        "    The neural network is a function from state space $R^{n_states}$ to action space $R^{n_actions}$\n",
        "    state->[agent(NN),policy]->action\n",
        "    \n",
        "    \"\"\" \n",
        "    def __init__(self, n_states, n_actions, hidden_size=32): \n",
        "        super(DDQN_Graph, self).__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.half_hidden_size = int(hidden_size/2)\n",
        "        self.dense_layer_1 = nn.Linear(n_states, hidden_size)\n",
        "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
        "        # V(s)\n",
        "        self.v_layer_1 = nn.Linear(hidden_size, self.half_hidden_size)\n",
        "        self.v_layer_2 = nn.Linear(self.half_hidden_size, 1)\n",
        "        # A(s, a)\n",
        "        self.a_layer_1 = nn.Linear(hidden_size, self.half_hidden_size)\n",
        "        self.a_layer_2 = nn.Linear(self.half_hidden_size, n_actions)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        # state: batch_size, state_size\n",
        "        # x: batch_size, hidden_size\n",
        "        #print(\"shape:\",state.shape)\n",
        "        x = F.relu(self.dense_layer_1(state))\n",
        "        # x: batch_size, hidden_size\n",
        "        x = F.relu(self.dense_layer_2(x))\n",
        "        # v: batch_size, half_hidden_size\n",
        "        v = F.relu(self.v_layer_1(x))\n",
        "        # v: batch_size, 1\n",
        "        v = self.v_layer_2(v)[0]\n",
        "        # a: batch_size, half_hidden_size\n",
        "        a = F.relu(self.a_layer_1(x))\n",
        "        # a: batch_size, action_size\n",
        "        a = self.a_layer_2(a)\n",
        "        \n",
        "        # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
        "        # shape of output of network: batch_size, action_size\n",
        "        return v + a - a.mean(dim=-1, keepdim=True).expand(-1, self.n_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Graph, LSTM"
      ],
      "metadata": {
        "id": "UAmHVEdXwfRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDQN_LSTM_Graph(nn.Module): \n",
        "    def __init__(self, n_states, n_actions, hidden_size=32): \n",
        "        super(DDQN_LSTM_Graph, self).__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.half_hidden_size = int(hidden_size/2)\n",
        "        self.dense_layer_1 = nn.LSTM(n_states, hidden_size)\n",
        "        self.dense_layer_2 = nn.LSTM(hidden_size, hidden_size)\n",
        "        # V(s)\n",
        "        self.v_layer_1 = nn.LSTM(hidden_size, self.half_hidden_size)\n",
        "        self.v_layer_2 = nn.LSTM(self.half_hidden_size, 1)\n",
        "        # A(s, a)\n",
        "        self.a_layer_1 = nn.LSTM(hidden_size, self.half_hidden_size)\n",
        "        self.a_layer_2 = nn.LSTM(self.half_hidden_size, n_actions)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        # state: batch_size, state_size\n",
        "        # x: batch_size, hidden_size\n",
        "        #print(\"shape:\",state.shape)\n",
        "        x = F.relu(self.dense_layer_1(state)[0])\n",
        "        # x: batch_size, hidden_size\n",
        "        x = F.relu(self.dense_layer_2(x)[0])\n",
        "        # v: batch_size, half_hidden_size\n",
        "        v = F.relu(self.v_layer_1(x)[0])\n",
        "        # v: batch_size, 1\n",
        "        v = self.v_layer_2(v)[0]\n",
        "        # a: batch_size, half_hidden_size\n",
        "        a = F.relu(self.a_layer_1(x)[0])\n",
        "        # a: batch_size, action_size\n",
        "        a = self.a_layer_2(a)[0]\n",
        "        \n",
        "        # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
        "        # shape of output of network: batch_size, action_size\n",
        "        return v + a - a.mean(dim=-1, keepdim=True).expand(-1, self.n_actions)"
      ],
      "metadata": {
        "id": "-v8h3RFwpDfB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LaCB0tevzzI"
      },
      "source": [
        "### Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-0prheTEvzzI"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory(): \n",
        "    \"\"\"\n",
        "    Replay memory records previous observations for the agent to learn later\n",
        "    by sampling from the memory randomly\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity): \n",
        "        super(ReplayMemory, self).__init__() \n",
        "        self.capacity = capacity\n",
        "        # to avoid empty memory list to insert transitions\n",
        "        self.memory = [None] * capacity\n",
        "        self.position = 0\n",
        "        self.Transition = namedtuple('Transition', \n",
        "                                     ('state', 'action', 'reward', 'next_state', 'done'))\n",
        "    \n",
        "    def size(self):\n",
        "        return len(self.memory) - self.memory.count(None)\n",
        "    \n",
        "    def push(self, *args):\n",
        "        # save a transition at a certain position of the memory\n",
        "        self.memory[self.position] = self.Transition(*args)        \n",
        "        # update position\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "    \n",
        "    def pull(self):\n",
        "        return [exp for exp in self.memory if exp is not None]\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        exps = random.sample(self.pull(), batch_size)\n",
        "        states = torch.tensor(np.vstack([e.state for e in exps if e is not None])).float()\n",
        "        actions = torch.tensor(np.vstack([e.action for e in exps if e is not None])).long()\n",
        "        rewards = torch.tensor(np.vstack([e.reward for e in exps if e is not None])).float()\n",
        "        next_states = torch.tensor(np.vstack([e.next_state for e in exps if e is not None])).float()\n",
        "        dones = torch.tensor(np.vstack([e.done for e in exps if e is not None]).astype(np.uint8)).float()\n",
        "        \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdlJegmpvzzJ"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XXOo_L4uvzzK"
      },
      "outputs": [],
      "source": [
        "class DDQN_Agent(): \n",
        "    \"\"\"docstring for ddqn_agent\"\"\"\n",
        "    def __init__(self, n_states, n_actions, batch_size, hidden_size, memory_size, \n",
        "                 update_step, learning_rate, gamma, tau):\n",
        "        super(DDQN_Agent, self).__init__()\n",
        "        # state space dimension\n",
        "        self.n_states = n_states\n",
        "        # action space dimension\n",
        "        self.n_actions = n_actions\n",
        "        # configuration\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.update_step = update_step\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        # check cpu or gpu\n",
        "        self.setup_gpu()\n",
        "        # initialize model graph\n",
        "        self.setup_model()\n",
        "        # initialize optimizer\n",
        "        self.setup_opt()\n",
        "        # enable Replay Memory\n",
        "        self.memory = ReplayMemory(memory_size)\n",
        "        # others\n",
        "        self.prepare_train()\n",
        "    \n",
        "    def setup_gpu(self): \n",
        "        self.device = torch.device('cpu') #I changed it to cpu due to error: data was on 2 devices because sometimes gpu was not available.\n",
        "    \n",
        "    def setup_model(self):\n",
        "      #choose linear or LSTM graph\n",
        "        self.policy_model = DDQN_LSTM_Graph(\n",
        "            self.n_states, \n",
        "            self.n_actions, \n",
        "            self.hidden_size).to(self.device)\n",
        "        self.target_model = DDQN_LSTM_Graph(\n",
        "            self.n_states, \n",
        "            self.n_actions, \n",
        "            self.hidden_size).to(self.device)\n",
        "    \n",
        "    def setup_opt(self):\n",
        "        self.opt = torch.optim.Adam(self.policy_model.parameters(), lr=self.lr)\n",
        "    \n",
        "    def prepare_train(self):\n",
        "        self.steps = 0\n",
        "    \n",
        "    def act(self, state, epsilon):\n",
        "        # take an action for a time step\n",
        "        # state: 1, state_size\n",
        "        #print(\"state: \",state)\n",
        "        state = torch.tensor(state).reshape(1, -1).to(self.device)\n",
        "       # print(\"actt\",state.shape)\n",
        "        # inference by policy model\n",
        "        self.policy_model.eval() #tell the model that you are testing here, and training has finished\n",
        "        with torch.no_grad():  #will make all the operations in the block have no gradients.: for reducing memory in test time, does it mean no wait update?\n",
        "            # action_vs: 1, action_size\n",
        "            action_vs = self.policy_model(state)\n",
        "            #print(\"could you take action?\")\n",
        "        self.policy_model.train()\n",
        "        # return action: 1\n",
        "        # epsilon greedy search\n",
        "        if np.random.random() > epsilon:\n",
        "            return np.argmax(action_vs.cpu().detach().numpy())\n",
        "        else:\n",
        "            return np.random.randint(self.n_actions)\n",
        "    \n",
        "    def step(self, s, a, r, s_, done):\n",
        "        # add one observation to memory\n",
        "        self.memory.push(s, a, r, s_, done)\n",
        "        # update model for every certain steps\n",
        "        self.steps = (self.steps + 1) % self.update_step\n",
        "        if self.steps == 0 and self.memory.size() >= self.batch_size:\n",
        "            exps = self.memory.sample(self.batch_size)\n",
        "            self.learn(exps)\n",
        "        else:\n",
        "            pass\n",
        "    \n",
        "    def learn(self, exps, soft_copy=True):\n",
        "        \n",
        "        for item in exps:\n",
        "            item.to(self.device)\n",
        "        # states: batch_size, state_size\n",
        "        # actions: batch_size, 1\n",
        "        # rewards: batch_size, 1\n",
        "        # next_states: batch_size, state_size\n",
        "        # dones: batch_size, 1\n",
        "        states, actions, rewards, next_states, dones = exps\n",
        "        # target side\n",
        "        _, next_idx = self.policy_model(next_states).detach().max(1) #maximum action values in batch of data, their index\n",
        "        # action values: batch_size, action_size\n",
        "        target_next_action_vs = self.target_model(next_states).detach().gather(1, next_idx.unsqueeze(1))\n",
        "        # Q values: batch_size, 1\n",
        "        # Q = reward + (gamma * Q[next state][next action]) for not done\n",
        "        target_q_vs = rewards + (self.gamma * target_next_action_vs * (1 - dones))\n",
        "        # policy side\n",
        "        # Q values: batch_size, 1\n",
        "        policy_q_vs = self.policy_model(states).gather(1, actions)\n",
        "        # compute MSE loss\n",
        "        loss = F.mse_loss(policy_q_vs, target_q_vs)\n",
        "        # update policy network\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "        # gradient clamping\n",
        "        for p in self.policy_model.parameters(): \n",
        "            p.grad.data.clamp_(-1, 1)\n",
        "        self.opt.step()\n",
        "        if soft_copy:\n",
        "            # update target network via soft copy with ratio tau\n",
        "            # θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "            for tp, lp in zip(self.target_model.parameters(), self.policy_model.parameters()):\n",
        "                tp.data.copy_(self.tau*lp.data + (1.0-self.tau)*tp.data)\n",
        "        else:\n",
        "            # update target network via hard copy\n",
        "            self.target_model.load_state_dict(self.policy_model.state_dict())\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Representation Learning and Adding Memory**"
      ],
      "metadata": {
        "id": "lzLkOldJYiCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tiles3 import tiles, IHT\n",
        "class Representation: \n",
        "  #make the features: agent's internal representations of state\n",
        "  #xt = u(xt-1,ot) a nonlinear function\n",
        "  #u can be fixed: tile coding, etc\n",
        "  #u can be learned: different RNNs \n",
        "  def __init__(self, hash_table_size=4096, num_tilings=8, num_tiles=8):\n",
        "    self.hash_table_size = hash_table_size\n",
        "    self.iht = IHT(hash_table_size)\n",
        "    self.num_tilings = num_tilings\n",
        "    self.num_tiles = num_tiles\n",
        "  def u(self,xt_1, ot):\n",
        "    #scale ot\n",
        "    Observation_High = [1.5, 1.5, 5., 5., 3.14, 5., 1., 1. ]\n",
        "    Observation_Low = [-1.5, -1.5, -5., -5., -3.14, -5., -0., -0. ]\n",
        "    ot_scaled = []\n",
        "    for i,element in enumerate(ot):\n",
        "      scaled_element = self.num_tiles*abs((ot[i] - Observation_Low[i])/(Observation_High[i] - Observation_Low[i]))\n",
        "      ot_scaled.append(scaled_element)\n",
        "    ot = torch.tensor(ot_scaled)\n",
        "    #scale xt_1\n",
        "    xt_1_scaled = []\n",
        "    for i,element in enumerate(xt_1):\n",
        "      scaled_element = self.num_tiles*abs(element/self.hash_table_size)\n",
        "      xt_1_scaled.append(scaled_element)\n",
        "    xt_1 = torch.tensor(xt_1_scaled)\n",
        "    #construct feature\n",
        "    new_feature =(xt_1,ot)\n",
        "    memory_trace = torch.concat(new_feature,0) #TODO: is there a better way\n",
        "    tile_coded = tiles(self.iht,self.num_tilings,memory_trace)\n",
        "    return np.array(tile_coded,dtype='f').reshape(self.num_tilings,) #list, 8(num_tilings) features, so no need to change num_states\n",
        "  "
      ],
      "metadata": {
        "id": "Oag0SszQYseY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VK0OTiqvzzP"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KQIkYFcWvzzP"
      },
      "outputs": [],
      "source": [
        "# initial definition\n",
        "batch_size = 64\n",
        "hidden_size = 64\n",
        "memory_size = int(1e5)\n",
        "update_step = 4\n",
        "learning_rate = 5e-4\n",
        "gamma = 0.99\n",
        "tau = 1e-2\n",
        "\n",
        "n_episodes = 1000\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "decay_rate = 0.004\n",
        "rewards_window_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvLSyTezvzzQ"
      },
      "outputs": [],
      "source": [
        "# initialize epsilon values for greedy search\n",
        "epsilon_array = np.zeros((n_episodes))\n",
        "for i in range(n_episodes):\n",
        "    epsilon = min_epsilon + (max_epsilon-min_epsilon)*np.exp(-decay_rate*i)\n",
        "    epsilon_array[i] = epsilon\n",
        "\n",
        "plt.plot(epsilon_array)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "F6Om9hPUvzzR"
      },
      "outputs": [],
      "source": [
        "# training process\n",
        "# initialize the DDQN agent given a configuration\n",
        "agent = DDQN_Agent(n_states, \n",
        "                   n_actions, \n",
        "                   batch_size, \n",
        "                   hidden_size, \n",
        "                   memory_size, \n",
        "                   update_step, \n",
        "                   learning_rate, \n",
        "                   gamma, \n",
        "                   tau)\n",
        "\n",
        "stack_agent = DDQN_Agent(2*n_states, \n",
        "                   n_actions, \n",
        "                   batch_size, \n",
        "                   hidden_size, \n",
        "                   memory_size, \n",
        "                   update_step, \n",
        "                   learning_rate, \n",
        "                   gamma, \n",
        "                   tau)\n",
        "\n",
        "def experiment_stack_previous_states(n_episodes, rewards_window_size, epsilon_array): #stacks the previous state with current state and use it as DDQN input\n",
        "    best_avg_rewards = 100.\n",
        "    total_rewards = []\n",
        "    rewards_deque = deque(maxlen=rewards_window_size)\n",
        "    t = trange(n_episodes)\n",
        "    for episode in t:\n",
        "        # initialize the state\n",
        "        cur_state = env.reset()\n",
        "        ############\n",
        "        next_augmented_state = np.zeros((16,),dtype = 'f')\n",
        "        augmented_state = np.zeros((16,),dtype = 'f')\n",
        "        prev_state = cur_state\n",
        "        ###########\n",
        "        done = False\n",
        "        rewards = 0\n",
        "        epsilon = epsilon_array[episode]\n",
        "        while not done:\n",
        "            #############################\n",
        "            augmented_state[0:8] = prev_state\n",
        "            augmented_state[8:]= cur_state\n",
        "            ###############################\n",
        "            action = stack_agent.act(augmented_state, epsilon)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_augmented_state[0:8] = cur_state\n",
        "            next_augmented_state[8:] = next_state\n",
        "            stack_agent.step(augmented_state, action, reward,next_augmented_state, done)\n",
        "            #######################\n",
        "            prev_state = cur_state\n",
        "            ######################\n",
        "            cur_state = next_state\n",
        "            rewards += reward\n",
        "        # update information\n",
        "        total_rewards.append(rewards)\n",
        "        rewards_deque.append(rewards)\n",
        "        avg_rewards = np.mean(rewards_deque)\n",
        "        t.set_description(\n",
        "            'Episode {} Epsilon {:.2f} Reward {:.2f} Avg_Reward {:.2f} Best_Avg_Reward {:.2f}'.format(\n",
        "                episode + 1, epsilon, rewards, avg_rewards, best_avg_rewards))\n",
        "        t.refresh()\n",
        "        # evaluation\n",
        "        if avg_rewards >= best_avg_rewards: \n",
        "            best_avg_rewards = avg_rewards\n",
        "            torch.save(agent.policy_model.state_dict(), DDQN_CHECKPOINT_PATH)\n",
        "        # the game is solved by earning more than +200 rewards for a single episode\n",
        "        if best_avg_rewards > 200:\n",
        "            break\n",
        "    return total_rewards, rewards_deque\n",
        "\n",
        "def experiment_representation(n_episodes, rewards_window_size, epsilon_array): #gets the idea from this formula: xt = u(xt-1,ot)\n",
        "    best_avg_rewards = 100.\n",
        "    total_rewards = []\n",
        "    rewards_deque = deque(maxlen=rewards_window_size)\n",
        "    t = trange(n_episodes)\n",
        "    for episode in t:\n",
        "        # initialize the state\n",
        "        cur_state = env.reset()\n",
        "        ############\n",
        "        xt_1 = np.copy(cur_state)\n",
        "        r =Representation()\n",
        "        ###########\n",
        "        done = False\n",
        "        rewards = 0\n",
        "        epsilon = epsilon_array[episode]\n",
        "        while not done:\n",
        "            #############################\n",
        "            ot = cur_state\n",
        "            xt = r.u(xt_1,ot)\n",
        "            ###############################\n",
        "            action = agent.act(xt, epsilon)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_xt = r.u(xt,next_state)\n",
        "            agent.step(cur_state, action, reward,next_xt, done)\n",
        "            #######################\n",
        "            xt_1 = xt\n",
        "            ######################\n",
        "            cur_state = next_state\n",
        "            rewards += reward\n",
        "        # update information\n",
        "        total_rewards.append(rewards)\n",
        "        rewards_deque.append(rewards)\n",
        "        avg_rewards = np.mean(rewards_deque)\n",
        "        t.set_description(\n",
        "            'Episode {} Epsilon {:.2f} Reward {:.2f} Avg_Reward {:.2f} Best_Avg_Reward {:.2f}'.format(\n",
        "                episode + 1, epsilon, rewards, avg_rewards, best_avg_rewards))\n",
        "        t.refresh()\n",
        "        # evaluation\n",
        "        if avg_rewards >= best_avg_rewards: \n",
        "            best_avg_rewards = avg_rewards\n",
        "            torch.save(agent.policy_model.state_dict(), DDQN_CHECKPOINT_PATH)\n",
        "        # the game is solved by earning more than +200 rewards for a single episode\n",
        "        if best_avg_rewards > 200:\n",
        "            break\n",
        "    return total_rewards, rewards_deque\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZ0UYW89vzzR",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "\n",
        "# a list of rewards for each episode\n",
        "# and a deque of rewards for latest episode given a certain rewards window size\n",
        "# training may take around 30 mins on CPU\n",
        "train_rewards, train_rewards_deque = experiment_representation(n_episodes, rewards_window_size, epsilon_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg4sOkXTvzzS"
      },
      "outputs": [],
      "source": [
        "# show rewards change in training\n",
        "plt.subplots(figsize = (5, 5), dpi=100)\n",
        "plt.plot(train_rewards)\n",
        "plt.ylabel('Total Reward', fontsize=12)\n",
        "plt.xlabel('Episode', fontsize=12)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.title('Total Rewards Per Training Episode', fontsize=12)\n",
        "# plt.savefig(DDQN_RESULT_IMG_PATH.format(0), dpi=100, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWGNO3d2vzzT"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOjXPA3PvzzT",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "test_episodes=100\n",
        "test_reward_array = np.zeros(100)\n",
        "agent = DDQN_Agent(n_states, \n",
        "                   n_actions, \n",
        "                   batch_size, \n",
        "                   hidden_size, \n",
        "                   memory_size, \n",
        "                   update_step, \n",
        "                   learning_rate, \n",
        "                   gamma, \n",
        "                   tau)\n",
        "# load check point to restore the model\n",
        "agent.policy_model.load_state_dict(\n",
        "    torch.load(DDQN_CHECKPOINT_PATH, map_location=agent.device))\n",
        "\n",
        "t = trange(test_episodes, leave=True)\n",
        "for episode in t: \n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    rewards = 0. \n",
        "    while not done: \n",
        "        # disable epsilon greedy search\n",
        "        action = agent.act(state, epsilon=0) \n",
        "        state, reward, done, _ = env.step(action)\n",
        "        rewards += reward\n",
        "    t.set_description('Episode {:.2f} Reward {:.2f}'.format(episode + 1, rewards))\n",
        "    t.refresh()\n",
        "    test_reward_array[episode] = rewards\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6NFZYF7vzzU"
      },
      "outputs": [],
      "source": [
        "avg_test_reward = round(np.mean(test_reward_array), 2)\n",
        "plt.subplots(figsize = (5, 5), dpi=100)\n",
        "plt.plot(test_reward_array)\n",
        "plt.ylabel('Total Reward', fontsize=12)\n",
        "plt.xlabel('Trial', fontsize=12)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.title('Total Rewards Per Trial for 100 Trials - Average: {:.2f}'.format(avg_test_reward), \n",
        "          fontsize=12)\n",
        "# plt.savefig(DDQN_RESULT_IMG_PATH.format(1), dpi=100, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s3YqQWBvzzU"
      },
      "outputs": [],
      "source": [
        "# take a look at how a DDQN agent act in the game\n",
        "# On OS X, you can install ffmpeg via `brew install ffmpeg`. \n",
        "# On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. \n",
        "# On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\n",
        "\n",
        "env = wrappers.Monitor(env, DDQN_AGENT_PATH, force=True)\n",
        "state = env.reset()\n",
        "done = False\n",
        "rewards = 0.\n",
        "while True:\n",
        "    # disable epsilon greedy search\n",
        "    action = agent.act(state, epsilon=0)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    rewards += reward\n",
        "    if done:\n",
        "        print('Total Rewards in this game: {:.2f}'.format(rewards))\n",
        "        break\n",
        "env.close()\n",
        "\n",
        "# find video record under ./output/ddqn_agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fZoKnOuvzzV"
      },
      "source": [
        "## Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnG1HQi5vzzV"
      },
      "source": [
        "1. Hasselt, H. V. (2010). Double Q-learning. In Advances in neural information processing systems (pp. 2613-2621).\n",
        "2. Van Hasselt, H., Guez, A., & Silver, D. (2016, March). Deep reinforcement learning with double q-learning. In Thirtieth AAAI conference on artificial intelligence.\n",
        "3. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}