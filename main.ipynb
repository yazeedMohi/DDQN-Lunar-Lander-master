{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDQN Lunar Lander\n",
    "Implementation of Double Deep Q-Network (DDQN) to solve the game Lunar Lander by earning more than +200 total reward on average over 100 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install numpy==1.22.0\n",
    "# %pip install gym==0.16.0\n",
    "# %pip install tqdm==4.43.0\n",
    "# %pip install matplotlib\n",
    "# %brew install swig\n",
    "# %pip install box2d-py\n",
    "# %pip install PyOpenGL PyOpenGL_accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependency\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path\n",
    "CURR_PATH = os.path.abspath('')\n",
    "OUTPUT_PATH = os.path.join(CURR_PATH, 'output')\n",
    "RANDOM_AGENT_PATH = os.path.join(OUTPUT_PATH, 'random_agent')\n",
    "DDQN_AGENT_PATH = os.path.join(OUTPUT_PATH, 'ddqn_agent')\n",
    "DDQN_CHECKPOINT_PATH = os.path.join(DDQN_AGENT_PATH, 'policy_model_checkpoint.pth')\n",
    "DDQN_RESULT_IMG_PATH = os.path.join(DDQN_AGENT_PATH, 'result_img_{}.png')\n",
    "\n",
    "for p in [RANDOM_AGENT_PATH, DDQN_AGENT_PATH]:\n",
    "    if not os.path.exists(p): \n",
    "        os.makedirs(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lunar Lander](res/Lunar_Lander.gif)\n",
    "<center>(https://gym.openai.com/envs/LunarLander-v2/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lunar Lander of OpenAI Gym is the game we want to solve in this work (Brockman et al. 2016). It is an interactive environment for an agent to land a rocket on a planet. A state here can be represented by an 8-dimensional continuous space:\n",
    "$$ (x, y, v_{x}, v_{y}, v_{\\theta}, leg_{left}, leg_{right}) $$\n",
    ", where $x$ and $y$ are the coordinates of the lander's position; $v_{x}$ and $v_{y}$ are the velocity components on two axes; $\\theta$ and $v_{\\theta}$ are the angle and the angular velocity separately; $leg_{left}$ and $leg_{right}$ are two binary values standing for whether the left or right leg of the lander is touching the ground.\n",
    "\n",
    "For each time step, there are four discrete actions available, that is, doing nothing, firing the left orientation engine, firing the main engine, and firing the right orientation engine.\n",
    "\n",
    "The game will be over or passed if the lander crashes or comes to rest. The reward for a bad ending is -100, while that for a happy ending is +100. The touching of the leg to the ground can generate a +10 reward, but each time of firing the main engine incurs a -0.3 penalty. Therefore, the total reward for a single episode ranges from 100 to more than 200 based on the final location of the lander on the pad. The distance between the landing pad and the lander will cause a penalization, which equals to the reward gained by moving closer to the pad. \n",
    "\n",
    "A method will be treated as a successful solution for this game if it can achieve more than +200 points average over 100 consecutive episodes.\n",
    "\n",
    "Note:\n",
    "1. six dimensional continuous state space with two more discrete variables\n",
    "2. discrete action space\n",
    "3. landing pad at coordinates $(0, 0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gym-box2d\n",
    "# %pip install box2d-py\n",
    "# %pip install box2d\n",
    "# %pip install gym\n",
    "# %pip install pygame\n",
    "# %pip install --upgrade swig\n",
    "\n",
    "# %pip install box2d box2d-kengz\n",
    "# %pip install gym==0.18.0\n",
    "# %pip install --upgrade git+http://github.com/pyglet/pyglet@pyglet-1.5-maintenance\n",
    "# %pip install gym --upgrade\n",
    "\n",
    "# %pip install pyglet==1.5.0\n",
    "# from pyglet.gl import *\n",
    "# print(\"imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state space: 8\n",
      "action space: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "n_states, n_actions = env.observation_space.shape[0], env.action_space.n\n",
    "print('state space: {}'.format(n_states))\n",
    "print('action space: {}'.format(n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "# env = wrappers.Monitor(env, RANDOM_AGENT_PATH, force=True)\n",
    "agent = RandomAgent(env.action_space)\n",
    "ob = env.reset()\n",
    "while True:\n",
    "    action = agent.act()\n",
    "    _, _, done, _, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_Graph(nn.Module): \n",
    "    def __init__(self, n_states, n_actions, hidden_size=32): \n",
    "        super(DDQN_Graph, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.half_hidden_size = int(hidden_size/2)\n",
    "        # hidden representation\n",
    "        self.dense_layer_1 = nn.Linear(n_states, hidden_size)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # V(s)\n",
    "        self.v_layer_1 = nn.Linear(hidden_size, self.half_hidden_size)\n",
    "        self.v_layer_2 = nn.Linear(self.half_hidden_size, 1)\n",
    "        # A(s, a)\n",
    "        self.a_layer_1 = nn.Linear(hidden_size, self.half_hidden_size)\n",
    "        self.a_layer_2 = nn.Linear(self.half_hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.dense_layer_1(state))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        v = F.relu(self.v_layer_1(x))\n",
    "        v = self.v_layer_2(v)\n",
    "        a = F.relu(self.a_layer_1(x))\n",
    "        a = self.a_layer_2(a)\n",
    "        \n",
    "        # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "        return v + a - a.mean(dim=-1, keepdim=True).expand(-1, self.n_actions)\n",
    "\n",
    "class DDQN_LSTM_Graph(nn.Module): \n",
    "    def __init__(self, n_states, n_actions, hidden_size=32): \n",
    "        super(DDQN_LSTM_Graph, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.half_hidden_size = int(hidden_size/2)\n",
    "        # hidden representation\n",
    "        self.lstm_layer_1 = nn.Linear(n_states, hidden_size)\n",
    "        self.lstm_layer_2 = nn.LSTM(hidden_size, hidden_size)\n",
    "        # V(s)\n",
    "        self.v_layer_1 = nn.Linear(hidden_size, self.half_hidden_size)\n",
    "        self.v_layer_2 = nn.Linear(self.half_hidden_size, 1)\n",
    "        # A(s, a)\n",
    "        self.a_layer_1 = nn.Linear(hidden_size, self.half_hidden_size)\n",
    "        self.a_layer_2 = nn.Linear(self.half_hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.lstm_layer_1(state))\n",
    "        x = F.relu(self.lstm_layer_2(x)[0])\n",
    "        v = F.relu(self.v_layer_1(x))\n",
    "        v = self.v_layer_2(v)\n",
    "        a = F.relu(self.a_layer_1(x))\n",
    "        a = self.a_layer_2(a)\n",
    "\n",
    "        # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "        return v + a - a.mean(dim=-1, keepdim=True).expand(-1, self.n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(): \n",
    "    \"\"\"\n",
    "    Replay memory records previous observations for the agent to learn later\n",
    "    by sampling from the memory randomly\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity): \n",
    "        super(ReplayMemory, self).__init__() \n",
    "        self.capacity = capacity\n",
    "        # to avoid empty memory list to insert transitions\n",
    "        self.memory = [None] * capacity\n",
    "        self.position = 0\n",
    "        self.Transition = namedtuple('Transition', \n",
    "                                     ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.memory) - self.memory.count(None)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # save a transition at a certain position of the memory\n",
    "        self.memory[self.position] = self.Transition(*args)        \n",
    "        # update position\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def pull(self):\n",
    "        return [exp for exp in self.memory if exp is not None]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        exps = random.sample(self.pull(), batch_size)\n",
    "        states = torch.tensor(np.vstack([e.state for e in exps if e is not None])).float()\n",
    "        actions = torch.tensor(np.vstack([e.action for e in exps if e is not None])).long()\n",
    "        rewards = torch.tensor(np.vstack([e.reward for e in exps if e is not None])).float()\n",
    "        next_states = torch.tensor(np.vstack([e.next_state for e in exps if e is not None])).float()\n",
    "        dones = torch.tensor(np.vstack([e.done for e in exps if e is not None]).astype(np.uint8)).float()\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_Agent(): \n",
    "    \"\"\"docstring for ddqn_agent\"\"\"\n",
    "    def __init__(self, n_states, n_actions, batch_size, hidden_size, memory_size, \n",
    "                 update_step, learning_rate, gamma, tau, mode = \"dense\"):\n",
    "        super(DDQN_Agent, self).__init__()\n",
    "        # state space dimension\n",
    "        self.n_states = n_states\n",
    "        # action space dimension\n",
    "        self.n_actions = n_actions\n",
    "        # configuration\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.update_step = update_step\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        # check cpu or gpu\n",
    "        self.setup_gpu()\n",
    "        # initialize model graph\n",
    "        self.setup_model(mode)\n",
    "        # initialize optimizer\n",
    "        self.setup_opt()\n",
    "        # enable Replay Memory\n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        # others\n",
    "        self.prepare_train()\n",
    "    \n",
    "    def setup_gpu(self): \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def setup_model(self, mode):\n",
    "        if(mode == \"dense\"):\n",
    "            self.policy_model = DDQN_Graph(\n",
    "                self.n_states, \n",
    "                self.n_actions, \n",
    "                self.hidden_size).to(self.device)\n",
    "            self.target_model = DDQN_Graph(\n",
    "                self.n_states, \n",
    "                self.n_actions, \n",
    "                self.hidden_size).to(self.device)\n",
    "        else:\n",
    "            self.policy_model = DDQN_LSTM_Graph(\n",
    "                self.n_states, \n",
    "                self.n_actions, \n",
    "                self.hidden_size).to(self.device)\n",
    "            self.target_model = DDQN_LSTM_Graph(\n",
    "                self.n_states, \n",
    "                self.n_actions, \n",
    "                self.hidden_size).to(self.device)\n",
    "\n",
    "    def setup_opt(self):\n",
    "        self.opt = torch.optim.Adam(self.policy_model.parameters(), lr=self.lr)\n",
    "    \n",
    "    def prepare_train(self):\n",
    "        self.steps = 0\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # take an action for a time step\n",
    "        # state: 1, state_size\n",
    "        state = torch.tensor(state).reshape(1, -1).to(self.device)\n",
    "        # print(state.shape)\n",
    "        # inference by policy model\n",
    "        self.policy_model.eval()\n",
    "        with torch.no_grad(): \n",
    "            # action_vs: 1, action_size\n",
    "            action_vs = self.policy_model(state)\n",
    "        self.policy_model.train()\n",
    "        # return action: 1\n",
    "        # epsilon greedy search\n",
    "        if np.random.random() > epsilon:\n",
    "            return np.argmax(action_vs.cpu().detach().numpy())\n",
    "        else:\n",
    "            return np.random.randint(self.n_actions)\n",
    "    \n",
    "    def step(self, s, a, r, s_, done):\n",
    "        # add one observation to memory\n",
    "        self.memory.push(s, a, r, s_, done)\n",
    "        # update model for every certain steps\n",
    "        self.steps = (self.steps + 1) % self.update_step\n",
    "        if self.steps == 0 and self.memory.size() >= self.batch_size:\n",
    "            exps = self.memory.sample(self.batch_size)\n",
    "            self.learn(exps)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def learn(self, exps, soft_copy=True):\n",
    "        \n",
    "        for item in exps:\n",
    "            item.to(self.device)\n",
    "        # states: batch_size, state_size\n",
    "        # actions: batch_size, 1\n",
    "        # rewards: batch_size, 1\n",
    "        # next_states: batch_size, state_size\n",
    "        # dones: batch_size, 1\n",
    "        states, actions, rewards, next_states, dones = exps\n",
    "        # target side\n",
    "        _, next_idx = self.policy_model(next_states).detach().max(1)\n",
    "        # action values: batch_size, action_size\n",
    "        target_next_action_vs = self.target_model(next_states).detach().gather(1, next_idx.unsqueeze(1))\n",
    "        # Q values: batch_size, 1\n",
    "        # Q = reward + (gamma * Q[next state][next action]) for not done\n",
    "        target_q_vs = rewards + (self.gamma * target_next_action_vs * (1 - dones))\n",
    "        # policy side\n",
    "        # Q values: batch_size, 1\n",
    "        policy_q_vs = self.policy_model(states).gather(1, actions)\n",
    "        # compute MSE loss\n",
    "        loss = F.mse_loss(policy_q_vs, target_q_vs)\n",
    "        # update policy network\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clamping\n",
    "        for p in self.policy_model.parameters(): \n",
    "            p.grad.data.clamp_(-1, 1)\n",
    "        self.opt.step()\n",
    "        if soft_copy:\n",
    "            # update target network via soft copy with ratio tau\n",
    "            # θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "            for tp, lp in zip(self.target_model.parameters(), self.policy_model.parameters()):\n",
    "                tp.data.copy_(self.tau*lp.data + (1.0-self.tau)*tp.data)\n",
    "        else:\n",
    "            # update target network via hard copy\n",
    "            self.target_model.load_state_dict(self.policy_model.state_dict())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial definition\n",
    "batch_size = 64\n",
    "hidden_size = 64\n",
    "memory_size = int(1e5)\n",
    "update_step = 4\n",
    "learning_rate = 5e-4\n",
    "gamma = 0.99\n",
    "tau = 1e-2\n",
    "\n",
    "n_episodes = 1000\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.004\n",
    "rewards_window_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize epsilon values for greedy search\n",
    "epsilon_array = np.zeros((n_episodes))\n",
    "for i in range(n_episodes):\n",
    "    epsilon = min_epsilon + (max_epsilon-min_epsilon)*np.exp(-decay_rate*i)\n",
    "    epsilon_array[i] = epsilon\n",
    "\n",
    "plt.plot(epsilon_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "# initialize the DDQN agent given a configuration\n",
    "from re import S\n",
    "\n",
    "\n",
    "agent = DDQN_Agent(n_states, \n",
    "                   n_actions, \n",
    "                   batch_size, \n",
    "                   hidden_size, \n",
    "                   memory_size, \n",
    "                   update_step, \n",
    "                   learning_rate, \n",
    "                   gamma, \n",
    "                   tau,\n",
    "                   \"lstm\")\n",
    "\n",
    "def ddqn_learn_op(n_episodes, rewards_window_size, epsilon_array):\n",
    "    best_avg_rewards = -np.inf\n",
    "    total_rewards = []\n",
    "    rewards_deque = deque(maxlen=rewards_window_size)\n",
    "    t = trange(n_episodes)\n",
    "    for episode in t:\n",
    "        # initialize the state\n",
    "        cur_state = env.reset()[0]\n",
    "        done = False\n",
    "        rewards = 0\n",
    "        epsilon = epsilon_array[episode]\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = agent.act(cur_state, epsilon)\n",
    "            s = env.step(action)\n",
    "            next_state, reward, done, trunc, _ = s\n",
    "            done = done or trunc\n",
    "            # print(s)\n",
    "            agent.step(cur_state, action, reward, next_state, done)\n",
    "            cur_state = next_state\n",
    "            rewards += reward\n",
    "            steps += 1\n",
    "            # print(done)\n",
    "            # stop\n",
    "        # update information\n",
    "        total_rewards.append(rewards)\n",
    "        rewards_deque.append(rewards)\n",
    "        avg_rewards = np.mean(rewards_deque)\n",
    "        t.set_description(\n",
    "            'Episode {} Epsilon {:.2f} Reward {:.2f} Avg_Reward {:.2f} Best_Avg_Reward {:.2f} Steps {}'.format(\n",
    "                episode + 1, epsilon, rewards, avg_rewards, best_avg_rewards, steps))\n",
    "        t.refresh()\n",
    "        # evaluation\n",
    "        if avg_rewards >= best_avg_rewards: \n",
    "            best_avg_rewards = avg_rewards\n",
    "            torch.save(agent.policy_model.state_dict(), DDQN_CHECKPOINT_PATH)\n",
    "        # the game is solved by earning more than +200 rewards for a single episode\n",
    "        if best_avg_rewards > 200:\n",
    "            break\n",
    "    return total_rewards, rewards_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1000 Epsilon 0.03 Reward -329.07 Avg_Reward -179.24 Best_Avg_Reward 73.78 Steps 391: 100%|██████████| 1000/1000 [29:24<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# a list of rewards for each episode\n",
    "# and a deque of rewards for latest episode given a certain rewards window size\n",
    "# training may take around 30 mins on CPU\n",
    "train_rewards, train_rewards_deque = ddqn_learn_op(n_episodes, rewards_window_size, epsilon_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show rewards change in training\n",
    "plt.subplots(figsize = (5, 5), dpi=100)\n",
    "plt.plot(train_rewards)\n",
    "plt.ylabel('Total Reward', fontsize=12)\n",
    "plt.xlabel('Episode', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('Total Rewards Per Training Episode', fontsize=12)\n",
    "# plt.savefig(DDQN_RESULT_IMG_PATH.format(0), dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_episodes=100\n",
    "test_reward_array = np.zeros(100)\n",
    "agent = DDQN_Agent(n_states, \n",
    "                   n_actions, \n",
    "                   batch_size, \n",
    "                   hidden_size, \n",
    "                   memory_size, \n",
    "                   update_step, \n",
    "                   learning_rate, \n",
    "                   gamma, \n",
    "                   tau)\n",
    "# load check point to restore the model\n",
    "agent.policy_model.load_state_dict(\n",
    "    torch.load(DDQN_CHECKPOINT_PATH, map_location=agent.device))\n",
    "\n",
    "t = trange(test_episodes, leave=True)\n",
    "for episode in t: \n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    rewards = 0. \n",
    "    while not done: \n",
    "        # disable epsilon greedy search\n",
    "        action = agent.act(state, epsilon=0) \n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        rewards += reward\n",
    "    t.set_description('Episode {:.2f} Reward {:.2f}'.format(episode + 1, rewards))\n",
    "    t.refresh()\n",
    "    test_reward_array[episode] = rewards\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_reward = round(np.mean(test_reward_array), 2)\n",
    "plt.subplots(figsize = (5, 5), dpi=100)\n",
    "plt.plot(test_reward_array)\n",
    "plt.ylabel('Total Reward', fontsize=12)\n",
    "plt.xlabel('Trial', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('Total Rewards Per Trial for 100 Trials - Average: {:.2f}'.format(avg_test_reward), \n",
    "          fontsize=12)\n",
    "# plt.savefig(DDQN_RESULT_IMG_PATH.format(1), dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at how a DDQN agent act in the game\n",
    "# On OS X, you can install ffmpeg via `brew install ffmpeg`. \n",
    "# On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. \n",
    "# On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\n",
    "\n",
    "env = wrappers.Monitor(env, DDQN_AGENT_PATH, force=True)\n",
    "state = env.reset()\n",
    "done = False\n",
    "rewards = 0.\n",
    "while True:\n",
    "    # disable epsilon greedy search\n",
    "    action = agent.act(state, epsilon=0)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    rewards += reward\n",
    "    if done:\n",
    "        print('Total Rewards in this game: {:.2f}'.format(rewards))\n",
    "        break\n",
    "env.close()\n",
    "\n",
    "# find video record under ./output/ddqn_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hasselt, H. V. (2010). Double Q-learning. In Advances in neural information processing systems (pp. 2613-2621).\n",
    "2. Van Hasselt, H., Guez, A., & Silver, D. (2016, March). Deep reinforcement learning with double q-learning. In Thirtieth AAAI conference on artificial intelligence.\n",
    "3. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "# env = gym.wrappers.Monitor(env, \"recording\")\n",
    "\n",
    "total_reward = 0.0\n",
    "total_steps = 0\n",
    "obs = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Episode done in %d steps, total reward %.2f\" % (\n",
    "    total_steps, total_reward))\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dm_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import dm_env\n",
    "from dm_env import specs\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class Stimulus:\n",
    "    def __init__(self, activation_length: int):\n",
    "        self.active = False\n",
    "        self.activation_length = activation_length\n",
    "        self.onset = None\n",
    "        self.activation_steps = 0\n",
    "\n",
    "    def set_onset(self, onset):\n",
    "        self.onset = onset\n",
    "\n",
    "    def tick(self, time_step):\n",
    "        if self.active:\n",
    "            if self.activation_steps >= self.activation_length:\n",
    "                self.active = False\n",
    "                self.activation_steps = 0\n",
    "            self.activation_steps += 1\n",
    "        else:\n",
    "            if self.onset == time_step:\n",
    "                self.active = True\n",
    "                self.activation_steps = 1\n",
    "\n",
    "    def get_value(self):\n",
    "        return int(self.active)\n",
    "\n",
    "\n",
    "class TraceConditioning(dm_env.Environment):\n",
    "    def __init__(self, seed: int, ISI_interval: Tuple[int, int], ITI_interval: Tuple[int, int], gamma: float,\n",
    "                 num_distractors: int, activation_lengths: dict):\n",
    "        self.num_US = 1\n",
    "        self.num_CS = 1\n",
    "\n",
    "        self.ISI_interval = ISI_interval\n",
    "        self.ITI_interval = ITI_interval\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.US = Stimulus(activation_lengths[\"US\"])\n",
    "        self.CS = Stimulus(activation_lengths[\"CS\"])\n",
    "\n",
    "        self.num_distractors = num_distractors\n",
    "        self.distractors_probs = 1. / np.arange(10, 110, 10)\n",
    "        self.distractors = [Stimulus(activation_lengths[\"distractor\"]) for _ in range(self.num_distractors)]\n",
    "\n",
    "        self.time_step = None\n",
    "        self.trial_start = None\n",
    "        self.rand_num_generator = np.random.RandomState(seed)\n",
    "\n",
    "    def reset(self):\n",
    "        self.time_step = 0\n",
    "        self.trial_start = 0\n",
    "        self.configure_trial()\n",
    "        self.configure_distractors()\n",
    "        self.tick()\n",
    "        return dm_env.restart(self.observation())\n",
    "\n",
    "    def step(self, _):\n",
    "        self.time_step += 1\n",
    "        if self.time_step == self.trial_start:\n",
    "            self.configure_trial()\n",
    "        self.configure_distractors()\n",
    "        self.tick()\n",
    "        return dm_env.TimeStep(dm_env.StepType.MID, self.cumulant(), self.gamma,\n",
    "                               self.observation())\n",
    "\n",
    "    def configure_trial(self):\n",
    "        self.CS.set_onset(self.time_step)\n",
    "        ISI = self.rand_num_generator.randint(self.ISI_interval[0], self.ISI_interval[1] + 1)\n",
    "        self.US.set_onset(self.time_step + ISI)\n",
    "        ITI = self.rand_num_generator.randint(self.ITI_interval[0], self.ITI_interval[1] + 1)\n",
    "        self.trial_start = self.time_step + ISI + ITI\n",
    "\n",
    "    def configure_distractors(self):\n",
    "        for d, distractor in enumerate(self.distractors):\n",
    "            if (not distractor.active) and self.rand_num_generator.rand() < self.distractors_probs[d]:\n",
    "                distractor.set_onset(self.time_step)\n",
    "\n",
    "    def tick(self):\n",
    "        self.US.tick(self.time_step)\n",
    "        self.CS.tick(self.time_step)\n",
    "        for d, distractor in enumerate(self.distractors):\n",
    "            distractor.tick(self.time_step)\n",
    "\n",
    "    def cumulant(self):\n",
    "        return self.US.get_value()\n",
    "\n",
    "    def observation(self):\n",
    "        observations = np.zeros(self.num_US + self.num_CS + self.num_distractors)\n",
    "        observations[0] = self.US.get_value()\n",
    "        observations[1] = self.CS.get_value()\n",
    "        for d, distractor in enumerate(self.distractors):\n",
    "            observations[d + self.num_US + self.num_CS] = distractor.get_value()\n",
    "        return observations\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return specs.BoundedArray(shape=(self.num_US + self.num_CS + self.num_distractors),\n",
    "                                  dtype=np.float32,\n",
    "                                  name=\"observation\",\n",
    "                                  minimum=0,\n",
    "                                  maximum=1)\n",
    "\n",
    "    def action_spec(self):\n",
    "        return specs.DiscreteArray(\n",
    "            dtype=int, num_values=0, name=\"action\")\n",
    "\n",
    "\n",
    "class TracePatterning(dm_env.Environment):\n",
    "    def __init__(\n",
    "            self, seed: int, ISI_interval: Tuple[int, int], ITI_interval: Tuple[int, int], gamma: float, num_CS: int,\n",
    "            num_activation_patterns: int, activation_patterns_prob: float, num_distractors: int,\n",
    "            activation_lengths: dict, noise: float):\n",
    "\n",
    "        self.num_US = 1\n",
    "        self.US = Stimulus(activation_lengths[\"US\"])\n",
    "\n",
    "        self.num_CS = num_CS\n",
    "        self.CSs = [Stimulus(activation_lengths[\"CS\"]) for _ in range(self.num_CS)]\n",
    "\n",
    "        self.num_distractors = num_distractors\n",
    "        self.distractors = [Stimulus(activation_lengths[\"distractor\"]) for _ in range(self.num_distractors)]\n",
    "\n",
    "        self.num_activation_patterns = num_activation_patterns\n",
    "        self.activation_patterns_prob = activation_patterns_prob\n",
    "        self.rand_num_generator = np.random.RandomState(seed)\n",
    "        self.activation_patterns = produce_activation_patterns(self.rand_num_generator, self.num_CS,\n",
    "                                                               self.num_activation_patterns)\n",
    "        self.p = ((2 ** self.num_CS) * self.activation_patterns_prob - self.num_activation_patterns) / \\\n",
    "                 (2 ** self.num_CS - self.num_activation_patterns)\n",
    "\n",
    "        self.ISI_interval = ISI_interval\n",
    "        self.ITI_interval = ITI_interval\n",
    "        self.gamma = gamma\n",
    "        self.noise = noise\n",
    "\n",
    "        self.time_step = None\n",
    "        self.trial_start = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.trial_start = 0\n",
    "        self.time_step = 0\n",
    "        self.configure_trial()\n",
    "        self.tick()\n",
    "        return dm_env.restart(self.observation())\n",
    "\n",
    "    def step(self, _):\n",
    "        self.time_step += 1\n",
    "        if self.time_step == self.trial_start:\n",
    "            self.configure_trial()\n",
    "        self.tick()\n",
    "        return dm_env.TimeStep(dm_env.StepType.MID, self.cumulant(), self.gamma,\n",
    "                               self.observation())\n",
    "\n",
    "    def configure_trial(self):\n",
    "        CS_pattern = self.set_CSs()\n",
    "        ISI = self.rand_num_generator.randint(self.ISI_interval[0], self.ISI_interval[1] + 1)\n",
    "        self.set_US(ISI, CS_pattern)\n",
    "        self.set_distractors()\n",
    "        ITI = self.rand_num_generator.randint(self.ITI_interval[0], self.ITI_interval[1] + 1)\n",
    "        self.trial_start = self.time_step + ISI + ITI\n",
    "\n",
    "    def set_CSs(self):\n",
    "        if self.rand_num_generator.rand() < self.p:\n",
    "            CS_pattern = self.activation_patterns[self.rand_num_generator.choice(self.num_activation_patterns), :]\n",
    "        else:\n",
    "            CS_pattern = np.ndarray.astype((self.rand_num_generator.randint(2, size=self.num_CS)), dtype=float)\n",
    "        for c, CS in enumerate(self.CSs):\n",
    "            if CS_pattern[c] == 1:\n",
    "                CS.set_onset(self.time_step)\n",
    "        return CS_pattern\n",
    "\n",
    "    def set_US(self, ISI, CS_pattern):\n",
    "        if np.sum(binary_match(CS_pattern, self.activation_patterns)) > 0:\n",
    "            if self.rand_num_generator.rand() > self.noise:\n",
    "                self.US.set_onset(self.time_step + ISI)\n",
    "        else:\n",
    "            if self.rand_num_generator.rand() < self.noise:\n",
    "                self.US.set_onset(self.time_step + ISI)\n",
    "\n",
    "    def set_distractors(self):\n",
    "        distractor_pattern = np.ndarray.astype((self.rand_num_generator.randint(2, size=self.num_distractors)),\n",
    "                                               dtype=float)\n",
    "        for d, distractor in enumerate(self.distractors):\n",
    "            if distractor_pattern[d]:\n",
    "                distractor.set_onset(self.time_step)\n",
    "\n",
    "    def tick(self):\n",
    "        for c, CS in enumerate(self.CSs):\n",
    "            CS.tick(self.time_step)\n",
    "        self.US.tick(self.time_step)\n",
    "        for d, distractor in enumerate(self.distractors):\n",
    "            distractor.tick(self.time_step)\n",
    "\n",
    "    def cumulant(self):\n",
    "        return self.US.get_value()\n",
    "\n",
    "    def observation(self):\n",
    "        observations = np.zeros(self.num_US + self.num_CS + self.num_distractors)\n",
    "        observations[0] = self.US.get_value()\n",
    "        for c, CS in enumerate(self.CSs):\n",
    "            observations[c + 1] = CS.get_value()\n",
    "        for d, distractor in enumerate(self.distractors):\n",
    "            observations[d + self.num_CS + self.num_US] = distractor.get_value()\n",
    "        return observations\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return specs.BoundedArray(shape=(self.num_US + self.num_CS + self.num_distractors),\n",
    "                                  dtype=np.float32,\n",
    "                                  name=\"observation\",\n",
    "                                  minimum=0,\n",
    "                                  maximum=1)\n",
    "\n",
    "    def action_spec(self):\n",
    "        return specs.DiscreteArray(\n",
    "            dtype=int, num_values=0, name=\"action\")\n",
    "\n",
    "\n",
    "class NoisyPatterning(TracePatterning):\n",
    "    def __init__(\n",
    "            self, seed: int, ISI_interval: Tuple[int, int], ITI_interval: Tuple[int, int], gamma: float, num_CS: int,\n",
    "            num_activation_patterns: int, activation_patterns_prob: float, num_distractors: int,\n",
    "            activation_lengths: dict, noise: float):\n",
    "        super().__init__(seed, ISI_interval, ITI_interval, gamma, num_CS, num_activation_patterns,\n",
    "                         activation_patterns_prob, num_distractors, activation_lengths, noise)\n",
    "        self.ISI_interval = (activation_lengths[\"CS\"], activation_lengths[\"CS\"])\n",
    "\n",
    "\n",
    "def compute_return_error(cumulants, predictions, gamma):\n",
    "    num_time_steps = len(cumulants)\n",
    "    returns = np.zeros(num_time_steps)\n",
    "    returns[-1] = cumulants[-1]\n",
    "    for t in range(num_time_steps - 2, -1, -1):\n",
    "        returns[t] = gamma * returns[t + 1] + cumulants[t]\n",
    "    return_error = (predictions - returns) ** 2\n",
    "    MSRE = return_error.mean()\n",
    "    return MSRE, return_error, returns\n",
    "\n",
    "\n",
    "def produce_activation_patterns(rand_num_generator, num_CS, num_activation_patterns):\n",
    "    activated_indices = list(itertools.combinations(np.arange(num_CS), int(num_CS / 2)))\n",
    "    selected_indices = rand_num_generator.choice(np.arange(len(activated_indices)), size=num_activation_patterns,\n",
    "                                                 replace=False)\n",
    "    activation_patterns = np.zeros((num_activation_patterns, num_CS))\n",
    "    for i in range(num_activation_patterns):\n",
    "        activation_patterns[i, activated_indices[selected_indices[i]]] = 1.0\n",
    "    return activation_patterns\n",
    "\n",
    "\n",
    "def binary_match(x, patterns):\n",
    "    if sum(x) == 0:\n",
    "        ones_match = np.ones(patterns.shape[0])\n",
    "    else:\n",
    "        ones_match = np.floor(np.dot(patterns, x) / sum(x))\n",
    "    if sum(1 - x) == 0:\n",
    "        zeros_match = np.ones(patterns.shape[0])\n",
    "    else:\n",
    "        zeros_match = np.floor(np.dot(1 - patterns, 1 - x) / sum(1 - x))\n",
    "    return ones_match * zeros_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
