{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDQN Lunar Lander\n",
    "Implementation of Double Deep Q-Network (DDQN) to solve the game Lunar Lander by earning more than +200 total reward on average over 100 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy==1.22.0\n",
    "# %pip install gym==0.18.0\n",
    "# %pip install tqdm==4.43.0\n",
    "# %pip install matplotlib\n",
    "# %brew install swig\n",
    "# %pip install box2d-py\n",
    "# %pip install PyOpenGL PyOpenGL_accelerate\n",
    "# %pip install box2d box2d-kengz\n",
    "# %pip install --upgrade git+http://github.com/pyglet/pyglet@pyglet-1.5-maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_PATH = os.path.abspath('')\n",
    "OUTPUT_PATH = os.path.join(CURR_PATH, 'output')\n",
    "RANDOM_AGENT_PATH = os.path.join(OUTPUT_PATH, 'random_agent')\n",
    "DDQN_AGENT_PATH = os.path.join(OUTPUT_PATH, 'ddqn_agent')\n",
    "DDQN_CHECKPOINT_PATH = os.path.join(DDQN_AGENT_PATH, 'policy_model_checkpoint.pth')\n",
    "DDQN_RESULT_IMG_PATH = os.path.join(DDQN_AGENT_PATH, 'result_img_{}.png')\n",
    "\n",
    "for p in [RANDOM_AGENT_PATH, DDQN_AGENT_PATH]: # add saving for other agents in here\n",
    "    if not os.path.exists(p): \n",
    "        os.makedirs(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "n_states, n_actions = env.observation_space.shape[0], env.action_space.n\n",
    "print('state space: {}'.format(n_states))\n",
    "print('action space: {}'.format(n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "# env = wrappers.Monitor(env, RANDOM_AGENT_PATH, force=True)\n",
    "agent = RandomAgent(env.action_space)\n",
    "ob = env.reset()\n",
    "while True:\n",
    "    action = agent.act()\n",
    "    _, _, done, _, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_Graph(nn.Module): \n",
    "    def __init__(self, n_states, n_actions, hidden_size=32): \n",
    "        super(DDQN_Graph, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.half_hidden_size = int(hidden_size/2)\n",
    "        # hidden representation\n",
    "        self.dense_layer_1 = nn.Linear(n_states, hidden_size)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # V(s)\n",
    "        self.v_layer_1 = nn.Linear(hidden_size, self.half_hidden_size)\n",
    "        self.v_layer_2 = nn.Linear(self.half_hidden_size, 1)\n",
    "        # A(s, a)\n",
    "        self.a_layer_1 = nn.Linear(hidden_size, self.half_hidden_size)\n",
    "        self.a_layer_2 = nn.Linear(self.half_hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.dense_layer_1(state))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        v = F.relu(self.v_layer_1(x))\n",
    "        v = self.v_layer_2(v)\n",
    "        a = F.relu(self.a_layer_1(x))\n",
    "        a = self.a_layer_2(a)\n",
    "        \n",
    "        # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "        return v + a - a.mean(dim=-1, keepdim=True).expand(-1, self.n_actions)\n",
    "\n",
    "class DDQN_LSTM_Graph(nn.Module): \n",
    "    def __init__(self, n_states, n_actions, hidden_size=32): \n",
    "        super(DDQN_LSTM_Graph, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.half_hidden_size = int(hidden_size/2)\n",
    "        # hidden representation\n",
    "        self.lstm_layer_1 = nn.Linear(n_states, hidden_size)\n",
    "        self.lstm_layer_2 = nn.LSTM(hidden_size, hidden_size)\n",
    "        # V(s)\n",
    "        self.v_layer_1 = nn.Linear(hidden_size, self.half_hidden_size)\n",
    "        self.v_layer_2 = nn.Linear(self.half_hidden_size, 1)\n",
    "        # A(s, a)\n",
    "        self.a_layer_1 = nn.Linear(hidden_size, self.half_hidden_size)\n",
    "        self.a_layer_2 = nn.Linear(self.half_hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.lstm_layer_1(state))\n",
    "        x = F.relu(self.lstm_layer_2(x)[0])\n",
    "        v = F.relu(self.v_layer_1(x))\n",
    "        v = self.v_layer_2(v)\n",
    "        a = F.relu(self.a_layer_1(x))\n",
    "        a = self.a_layer_2(a)\n",
    "\n",
    "        # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "        return v + a - a.mean(dim=-1, keepdim=True).expand(-1, self.n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(): \n",
    "    \"\"\"\n",
    "    Replay memory records previous observations for the agent to learn later\n",
    "    by sampling from the memory randomly\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity): \n",
    "        super(ReplayMemory, self).__init__() \n",
    "        self.capacity = capacity\n",
    "        # to avoid empty memory list to insert transitions\n",
    "        self.memory = [None] * capacity\n",
    "        self.position = 0\n",
    "        self.Transition = namedtuple('Transition', \n",
    "                                     ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.memory) - self.memory.count(None)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # save a transition at a certain position of the memory\n",
    "        self.memory[self.position] = self.Transition(*args)        \n",
    "        # update position\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def pull(self):\n",
    "        return [exp for exp in self.memory if exp is not None]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        exps = random.sample(self.pull(), batch_size)\n",
    "        states = torch.tensor(np.vstack([e.state for e in exps if e is not None])).float()\n",
    "        actions = torch.tensor(np.vstack([e.action for e in exps if e is not None])).long()\n",
    "        rewards = torch.tensor(np.vstack([e.reward for e in exps if e is not None])).float()\n",
    "        next_states = torch.tensor(np.vstack([e.next_state for e in exps if e is not None])).float()\n",
    "        dones = torch.tensor(np.vstack([e.done for e in exps if e is not None]).astype(np.uint8)).float()\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_Agent(): \n",
    "    \"\"\"docstring for ddqn_agent\"\"\"\n",
    "    def __init__(self, n_states, n_actions, batch_size, hidden_size, memory_size, \n",
    "                 update_step, learning_rate, gamma, tau, mode = \"dense\"):\n",
    "        super(DDQN_Agent, self).__init__()\n",
    "        # state space dimension\n",
    "        self.n_states = n_states\n",
    "        # action space dimension\n",
    "        self.n_actions = n_actions\n",
    "        # configuration\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.update_step = update_step\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        # check cpu or gpu\n",
    "        self.setup_gpu()\n",
    "        # initialize model graph\n",
    "        self.setup_model(mode)\n",
    "        # initialize optimizer\n",
    "        self.setup_opt()\n",
    "        # enable Replay Memory\n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        # others\n",
    "        self.prepare_train()\n",
    "    \n",
    "    def setup_gpu(self): \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def setup_model(self, mode):\n",
    "        if(mode == \"dense\"):\n",
    "            self.policy_model = DDQN_Graph(\n",
    "                self.n_states, \n",
    "                self.n_actions, \n",
    "                self.hidden_size).to(self.device)\n",
    "            self.target_model = DDQN_Graph(\n",
    "                self.n_states, \n",
    "                self.n_actions, \n",
    "                self.hidden_size).to(self.device)\n",
    "        else:\n",
    "            self.policy_model = DDQN_LSTM_Graph(\n",
    "                self.n_states, \n",
    "                self.n_actions, \n",
    "                self.hidden_size).to(self.device)\n",
    "            self.target_model = DDQN_LSTM_Graph(\n",
    "                self.n_states, \n",
    "                self.n_actions, \n",
    "                self.hidden_size).to(self.device)\n",
    "\n",
    "    def setup_opt(self):\n",
    "        self.opt = torch.optim.Adam(self.policy_model.parameters(), lr=self.lr)\n",
    "    \n",
    "    def prepare_train(self):\n",
    "        self.steps = 0\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # take an action for a time step\n",
    "        # state: 1, state_size\n",
    "        state = torch.tensor(state).reshape(1, -1).to(self.device)\n",
    "        # print(state.shape)\n",
    "        # inference by policy model\n",
    "        self.policy_model.eval()\n",
    "        with torch.no_grad(): \n",
    "            # action_vs: 1, action_size\n",
    "            action_vs = self.policy_model(state)\n",
    "        self.policy_model.train()\n",
    "        # return action: 1\n",
    "        # epsilon greedy search\n",
    "        if np.random.random() > epsilon:\n",
    "            return np.argmax(action_vs.cpu().detach().numpy())\n",
    "        else:\n",
    "            return np.random.randint(self.n_actions)\n",
    "    \n",
    "    def step(self, s, a, r, s_, done):\n",
    "        # add one observation to memory\n",
    "        self.memory.push(s, a, r, s_, done)\n",
    "        # update model for every certain steps\n",
    "        self.steps = (self.steps + 1) % self.update_step\n",
    "        if self.steps == 0 and self.memory.size() >= self.batch_size:\n",
    "            exps = self.memory.sample(self.batch_size)\n",
    "            self.learn(exps)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def learn(self, exps, soft_copy=True):\n",
    "        \n",
    "        for item in exps:\n",
    "            item.to(self.device)\n",
    "        # states: batch_size, state_size\n",
    "        # actions: batch_size, 1\n",
    "        # rewards: batch_size, 1\n",
    "        # next_states: batch_size, state_size\n",
    "        # dones: batch_size, 1\n",
    "        states, actions, rewards, next_states, dones = exps\n",
    "        # target side\n",
    "        _, next_idx = self.policy_model(next_states).detach().max(1)\n",
    "        # action values: batch_size, action_size\n",
    "        target_next_action_vs = self.target_model(next_states).detach().gather(1, next_idx.unsqueeze(1))\n",
    "        # Q values: batch_size, 1\n",
    "        # Q = reward + (gamma * Q[next state][next action]) for not done\n",
    "        target_q_vs = rewards + (self.gamma * target_next_action_vs * (1 - dones))\n",
    "        # policy side\n",
    "        # Q values: batch_size, 1\n",
    "        policy_q_vs = self.policy_model(states).gather(1, actions)\n",
    "        # compute MSE loss\n",
    "        loss = F.mse_loss(policy_q_vs, target_q_vs)\n",
    "        # update policy network\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clamping\n",
    "        for p in self.policy_model.parameters(): \n",
    "            p.grad.data.clamp_(-1, 1)\n",
    "        self.opt.step()\n",
    "        if soft_copy:\n",
    "            # update target network via soft copy with ratio tau\n",
    "            # θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "            for tp, lp in zip(self.target_model.parameters(), self.policy_model.parameters()):\n",
    "                tp.data.copy_(self.tau * lp.data + (1.0-self.tau) * tp.data)\n",
    "        else:\n",
    "            # update target network via hard copy\n",
    "            self.target_model.load_state_dict(self.policy_model.state_dict())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial definition\n",
    "batch_size = 64\n",
    "hidden_size = 64\n",
    "memory_size = int(1e5)\n",
    "update_step = 4\n",
    "learning_rate = 5e-4\n",
    "gamma = 0.99\n",
    "tau = 1e-2\n",
    "\n",
    "n_episodes = 1000\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.004\n",
    "rewards_window_size = 100\n",
    "\n",
    "lambda_decay = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize epsilon values for greedy search\n",
    "epsilon_array = np.zeros((n_episodes))\n",
    "for i in range(n_episodes):\n",
    "    epsilon = min_epsilon + (max_epsilon-min_epsilon)*np.exp(-decay_rate*i)\n",
    "    epsilon_array[i] = epsilon\n",
    "\n",
    "plt.plot(epsilon_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "# initialize the DDQN agent given a configuration\n",
    "from re import S\n",
    "\n",
    "\n",
    "agent = DDQN_Agent(n_states, \n",
    "                   n_actions, \n",
    "                   batch_size, \n",
    "                   hidden_size, \n",
    "                   memory_size, \n",
    "                   update_step, \n",
    "                   learning_rate, \n",
    "                   gamma, \n",
    "                   tau,\n",
    "                   \"dense\")\n",
    "\n",
    "def ddqn_learn_op(n_episodes, rewards_window_size, epsilon_array):\n",
    "    best_avg_rewards = -np.inf\n",
    "    total_rewards = []\n",
    "    rewards_deque = deque(maxlen=rewards_window_size)\n",
    "    t = trange(n_episodes)\n",
    "    cur_state = np.zeros(n_states)\n",
    "    for episode in t:\n",
    "        # initialize the state\n",
    "        cur_state = [(lambda_decay * x + (1 - lambda_decay) * y).astype(np.float32) for x, y in zip(cur_state, env.reset()[0])]\n",
    "        done = False\n",
    "        rewards = 0\n",
    "        epsilon = epsilon_array[episode]\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = agent.act(cur_state, epsilon)\n",
    "            s = env.step(action)\n",
    "            next_state, reward, done, trunc, _ = s\n",
    "            done = done or trunc\n",
    "            # print(s)\n",
    "            agent.step(cur_state, action, reward, next_state, done)\n",
    "            \n",
    "            # cur_state = next_state\n",
    "            cur_state = [(lambda_decay * x + (1 - lambda_decay) * y).astype(np.float32) for x, y in zip(cur_state, next_state)]\n",
    "\n",
    "            rewards += reward\n",
    "            steps += 1\n",
    "            # print(done)\n",
    "            # stop\n",
    "        # update information\n",
    "        total_rewards.append(rewards)\n",
    "        rewards_deque.append(rewards)\n",
    "        avg_rewards = np.mean(rewards_deque)\n",
    "        t.set_description(\n",
    "            'Episode {} Epsilon {:.2f} Reward {:.2f} Avg_Reward {:.2f} Best_Avg_Reward {:.2f} Steps {}'.format(\n",
    "                episode + 1, epsilon, rewards, avg_rewards, best_avg_rewards, steps))\n",
    "        t.refresh()\n",
    "        # evaluation\n",
    "        # if avg_rewards >= best_avg_rewards: \n",
    "        #     best_avg_rewards = avg_rewards\n",
    "        #     torch.save(agent.policy_model.state_dict(), DDQN_CHECKPOINT_PATH)\n",
    "        # the game is solved by earning more than +200 rewards for a single episode\n",
    "        # if best_avg_rewards > 200:\n",
    "        #     break\n",
    "    return total_rewards, rewards_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a list of rewards for each episode\n",
    "# and a deque of rewards for latest episode given a certain rewards window size\n",
    "# training may take around 30 mins on CPU\n",
    "train_rewards, train_rewards_deque = ddqn_learn_op(n_episodes, rewards_window_size, epsilon_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rewards = np.array([ddqn_learn_op(n_episodes, rewards_window_size, epsilon_array)[0] for _ in range(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show rewards change in training\n",
    "# for i in range(len(train_rewards)):\n",
    "    # if(len(train_rewards[i]) < n_episodes+100):  train_rewards[i] = np.concatenate((np.array(train_rewards[i]), np.repeat(200, 90 + n_episodes - len(train_rewards[i]))))#[-1:n_episodes+1000] = 200\n",
    "\n",
    "expanded_train_rewards = np.array([rew if len(rew) >= n_episodes else np.concatenate((rew, np.repeat(200, n_episodes - len(rew)))) for rew in train_rewards])\n",
    "plt.subplots(figsize = (5, 5), dpi=100)\n",
    "y = np.mean(expanded_train_rewards, axis=0)\n",
    "ci = 1.96 * np.std(expanded_train_rewards, axis=0)/np.sqrt(len(expanded_train_rewards[0]))\n",
    "\n",
    "plt.plot(y)\n",
    "plt.fill_between(range(len(expanded_train_rewards[0])), (y-ci), (y+ci), color='b', alpha=.1)\n",
    "\n",
    "plt.ylabel('Total Reward', fontsize=12)\n",
    "plt.xlabel('Episode', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('Total Rewards Per Training Episode', fontsize=12)\n",
    "plt.savefig(DDQN_RESULT_IMG_PATH.format(0), dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_episodes=100\n",
    "test_reward_array = np.zeros(100)\n",
    "agent = DDQN_Agent(n_states, \n",
    "                   n_actions, \n",
    "                   batch_size, \n",
    "                   hidden_size, \n",
    "                   memory_size, \n",
    "                   update_step, \n",
    "                   learning_rate, \n",
    "                   gamma, \n",
    "                   tau)\n",
    "# load check point to restore the model\n",
    "agent.policy_model.load_state_dict(\n",
    "    torch.load(DDQN_CHECKPOINT_PATH, map_location=agent.device))\n",
    "\n",
    "t = trange(test_episodes, leave=True)\n",
    "for episode in t: \n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    rewards = 0. \n",
    "    while not done: \n",
    "        # disable epsilon greedy search\n",
    "        action = agent.act(state, epsilon=0) \n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        rewards += reward\n",
    "    t.set_description('Episode {:.2f} Reward {:.2f}'.format(episode + 1, rewards))\n",
    "    t.refresh()\n",
    "    test_reward_array[episode] = rewards\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_reward = round(np.mean(test_reward_array), 2)\n",
    "plt.subplots(figsize = (5, 5), dpi=100)\n",
    "plt.plot(test_reward_array)\n",
    "plt.ylabel('Total Reward', fontsize=12)\n",
    "plt.xlabel('Trial', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('Total Rewards Per Trial for 100 Trials - Average: {:.2f}'.format(avg_test_reward), \n",
    "          fontsize=12)\n",
    "# plt.savefig(DDQN_RESULT_IMG_PATH.format(1), dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at how a DDQN agent act in the game\n",
    "# On OS X, you can install ffmpeg via `brew install ffmpeg`. \n",
    "# On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. \n",
    "# On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\n",
    "\n",
    "env = wrappers.Monitor(env, DDQN_AGENT_PATH, force=True)\n",
    "state = env.reset()\n",
    "done = False\n",
    "rewards = 0.\n",
    "while True:\n",
    "    # disable epsilon greedy search\n",
    "    action = agent.act(state, epsilon=0)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    rewards += reward\n",
    "    if done:\n",
    "        print('Total Rewards in this game: {:.2f}'.format(rewards))\n",
    "        break\n",
    "env.close()\n",
    "\n",
    "# find video record under ./output/ddqn_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hasselt, H. V. (2010). Double Q-learning. In Advances in neural information processing systems (pp. 2613-2621).\n",
    "2. Van Hasselt, H., Guez, A., & Silver, D. (2016, March). Deep reinforcement learning with double q-learning. In Thirtieth AAAI conference on artificial intelligence.\n",
    "3. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "\n",
    "\n",
    "agent = DDQN_Agent(n_states, \n",
    "                   n_actions, \n",
    "                   batch_size, \n",
    "                   hidden_size, \n",
    "                   memory_size, \n",
    "                   update_step, \n",
    "                   learning_rate, \n",
    "                   gamma, \n",
    "                   tau)\n",
    "# load check point to restore the model\n",
    "agent.policy_model.load_state_dict(\n",
    "    torch.load(DDQN_CHECKPOINT_PATH, map_location=agent.device))\n",
    "    \n",
    "total_reward = 0.0\n",
    "total_steps = 0\n",
    "obs = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = agent.act(state, epsilon=0)\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Episode done in %d steps, total reward %.2f.\\nSaved recording to %s\" % (\n",
    "    total_steps, total_reward, DDQN_RESULT_IMG_PATH))\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install dm_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import dm_env\n",
    "from dm_env import specs\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class Stimulus:\n",
    "    def __init__(self, activation_length: int):\n",
    "        self.active = False\n",
    "        self.activation_length = activation_length\n",
    "        self.onset = None\n",
    "        self.activation_steps = 0\n",
    "\n",
    "    def set_onset(self, onset):\n",
    "        self.onset = onset\n",
    "\n",
    "    def tick(self, time_step):\n",
    "        if self.active:\n",
    "            if self.activation_steps >= self.activation_length:\n",
    "                self.active = False\n",
    "                self.activation_steps = 0\n",
    "            self.activation_steps += 1\n",
    "        else:\n",
    "            if self.onset == time_step:\n",
    "                self.active = True\n",
    "                self.activation_steps = 1\n",
    "\n",
    "    def get_value(self):\n",
    "        return int(self.active)\n",
    "\n",
    "\n",
    "class TraceConditioning(dm_env.Environment):\n",
    "    def __init__(self, seed: int, ISI_interval: Tuple[int, int], ITI_interval: Tuple[int, int], gamma: float,\n",
    "                 num_distractors: int, activation_lengths: dict):\n",
    "        self.num_US = 1\n",
    "        self.num_CS = 1\n",
    "\n",
    "        self.ISI_interval = ISI_interval\n",
    "        self.ITI_interval = ITI_interval\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.US = Stimulus(activation_lengths[\"US\"])\n",
    "        self.CS = Stimulus(activation_lengths[\"CS\"])\n",
    "\n",
    "        self.num_distractors = num_distractors\n",
    "        self.distractors_probs = 1. / np.arange(10, 110, 10)\n",
    "        self.distractors = [Stimulus(activation_lengths[\"distractor\"]) for _ in range(self.num_distractors)]\n",
    "\n",
    "        self.time_step = None\n",
    "        self.trial_start = None\n",
    "        self.rand_num_generator = np.random.RandomState(seed)\n",
    "\n",
    "    def reset(self):\n",
    "        self.time_step = 0\n",
    "        self.trial_start = 0\n",
    "        self.configure_trial()\n",
    "        self.configure_distractors()\n",
    "        self.tick()\n",
    "        return dm_env.restart(self.observation())\n",
    "\n",
    "    def step(self, _):\n",
    "        self.time_step += 1\n",
    "        if self.time_step == self.trial_start:\n",
    "            self.configure_trial()\n",
    "        self.configure_distractors()\n",
    "        self.tick()\n",
    "        return dm_env.TimeStep(dm_env.StepType.MID, self.cumulant(), self.gamma,\n",
    "                               self.observation())\n",
    "\n",
    "    def configure_trial(self):\n",
    "        self.CS.set_onset(self.time_step)\n",
    "        ISI = self.rand_num_generator.randint(self.ISI_interval[0], self.ISI_interval[1] + 1)\n",
    "        self.US.set_onset(self.time_step + ISI)\n",
    "        ITI = self.rand_num_generator.randint(self.ITI_interval[0], self.ITI_interval[1] + 1)\n",
    "        self.trial_start = self.time_step + ISI + ITI\n",
    "\n",
    "    def configure_distractors(self):\n",
    "        for d, distractor in enumerate(self.distractors):\n",
    "            if (not distractor.active) and self.rand_num_generator.rand() < self.distractors_probs[d]:\n",
    "                distractor.set_onset(self.time_step)\n",
    "\n",
    "    def tick(self):\n",
    "        self.US.tick(self.time_step)\n",
    "        self.CS.tick(self.time_step)\n",
    "        for d, distractor in enumerate(self.distractors):\n",
    "            distractor.tick(self.time_step)\n",
    "\n",
    "    def cumulant(self):\n",
    "        return self.US.get_value()\n",
    "\n",
    "    def observation(self):\n",
    "        observations = np.zeros(self.num_US + self.num_CS + self.num_distractors)\n",
    "        observations[0] = self.US.get_value()\n",
    "        observations[1] = self.CS.get_value()\n",
    "        for d, distractor in enumerate(self.distractors):\n",
    "            observations[d + self.num_US + self.num_CS] = distractor.get_value()\n",
    "        return observations\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return specs.BoundedArray(shape=(self.num_US + self.num_CS + self.num_distractors),\n",
    "                                  dtype=np.float32,\n",
    "                                  name=\"observation\",\n",
    "                                  minimum=0,\n",
    "                                  maximum=1)\n",
    "\n",
    "    def action_spec(self):\n",
    "        return specs.DiscreteArray(\n",
    "            dtype=int, num_values=0, name=\"action\")\n",
    "\n",
    "\n",
    "class TracePatterning(dm_env.Environment):\n",
    "    def __init__(\n",
    "            self, seed: int, ISI_interval: Tuple[int, int], ITI_interval: Tuple[int, int], gamma: float, num_CS: int,\n",
    "            num_activation_patterns: int, activation_patterns_prob: float, num_distractors: int,\n",
    "            activation_lengths: dict, noise: float):\n",
    "\n",
    "        self.num_US = 1\n",
    "        self.US = Stimulus(activation_lengths[\"US\"])\n",
    "\n",
    "        self.num_CS = num_CS\n",
    "        self.CSs = [Stimulus(activation_lengths[\"CS\"]) for _ in range(self.num_CS)]\n",
    "\n",
    "        self.num_distractors = num_distractors\n",
    "        self.distractors = [Stimulus(activation_lengths[\"distractor\"]) for _ in range(self.num_distractors)]\n",
    "\n",
    "        self.num_activation_patterns = num_activation_patterns\n",
    "        self.activation_patterns_prob = activation_patterns_prob\n",
    "        self.rand_num_generator = np.random.RandomState(seed)\n",
    "        self.activation_patterns = produce_activation_patterns(self.rand_num_generator, self.num_CS,\n",
    "                                                               self.num_activation_patterns)\n",
    "        self.p = ((2 ** self.num_CS) * self.activation_patterns_prob - self.num_activation_patterns) / \\\n",
    "                 (2 ** self.num_CS - self.num_activation_patterns)\n",
    "\n",
    "        self.ISI_interval = ISI_interval\n",
    "        self.ITI_interval = ITI_interval\n",
    "        self.gamma = gamma\n",
    "        self.noise = noise\n",
    "\n",
    "        self.time_step = None\n",
    "        self.trial_start = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.trial_start = 0\n",
    "        self.time_step = 0\n",
    "        self.configure_trial()\n",
    "        self.tick()\n",
    "        return dm_env.restart(self.observation())\n",
    "\n",
    "    def step(self, _):\n",
    "        self.time_step += 1\n",
    "        if self.time_step == self.trial_start:\n",
    "            self.configure_trial()\n",
    "        self.tick()\n",
    "        return dm_env.TimeStep(dm_env.StepType.MID, self.cumulant(), self.gamma,\n",
    "                               self.observation())\n",
    "\n",
    "    def configure_trial(self):\n",
    "        CS_pattern = self.set_CSs()\n",
    "        ISI = self.rand_num_generator.randint(self.ISI_interval[0], self.ISI_interval[1] + 1)\n",
    "        self.set_US(ISI, CS_pattern)\n",
    "        self.set_distractors()\n",
    "        ITI = self.rand_num_generator.randint(self.ITI_interval[0], self.ITI_interval[1] + 1)\n",
    "        self.trial_start = self.time_step + ISI + ITI\n",
    "\n",
    "    def set_CSs(self):\n",
    "        if self.rand_num_generator.rand() < self.p:\n",
    "            CS_pattern = self.activation_patterns[self.rand_num_generator.choice(self.num_activation_patterns), :]\n",
    "        else:\n",
    "            CS_pattern = np.ndarray.astype((self.rand_num_generator.randint(2, size=self.num_CS)), dtype=float)\n",
    "        for c, CS in enumerate(self.CSs):\n",
    "            if CS_pattern[c] == 1:\n",
    "                CS.set_onset(self.time_step)\n",
    "        return CS_pattern\n",
    "\n",
    "    def set_US(self, ISI, CS_pattern):\n",
    "        if np.sum(binary_match(CS_pattern, self.activation_patterns)) > 0:\n",
    "            if self.rand_num_generator.rand() > self.noise:\n",
    "                self.US.set_onset(self.time_step + ISI)\n",
    "        else:\n",
    "            if self.rand_num_generator.rand() < self.noise:\n",
    "                self.US.set_onset(self.time_step + ISI)\n",
    "\n",
    "    def set_distractors(self):\n",
    "        distractor_pattern = np.ndarray.astype((self.rand_num_generator.randint(2, size=self.num_distractors)),\n",
    "                                               dtype=float)\n",
    "        for d, distractor in enumerate(self.distractors):\n",
    "            if distractor_pattern[d]:\n",
    "                distractor.set_onset(self.time_step)\n",
    "\n",
    "    def tick(self):\n",
    "        for c, CS in enumerate(self.CSs):\n",
    "            CS.tick(self.time_step)\n",
    "        self.US.tick(self.time_step)\n",
    "        for d, distractor in enumerate(self.distractors):\n",
    "            distractor.tick(self.time_step)\n",
    "\n",
    "    def cumulant(self):\n",
    "        return self.US.get_value()\n",
    "\n",
    "    def observation(self):\n",
    "        observations = np.zeros(self.num_US + self.num_CS + self.num_distractors)\n",
    "        observations[0] = self.US.get_value()\n",
    "        for c, CS in enumerate(self.CSs):\n",
    "            observations[c + 1] = CS.get_value()\n",
    "        for d, distractor in enumerate(self.distractors):\n",
    "            observations[d + self.num_CS + self.num_US] = distractor.get_value()\n",
    "        return observations\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return specs.BoundedArray(shape=(self.num_US + self.num_CS + self.num_distractors),\n",
    "                                  dtype=np.float32,\n",
    "                                  name=\"observation\",\n",
    "                                  minimum=0,\n",
    "                                  maximum=1)\n",
    "\n",
    "    def action_spec(self):\n",
    "        return specs.DiscreteArray(\n",
    "            dtype=int, num_values=0, name=\"action\")\n",
    "\n",
    "\n",
    "class NoisyPatterning(TracePatterning):\n",
    "    def __init__(\n",
    "            self, seed: int, ISI_interval: Tuple[int, int], ITI_interval: Tuple[int, int], gamma: float, num_CS: int,\n",
    "            num_activation_patterns: int, activation_patterns_prob: float, num_distractors: int,\n",
    "            activation_lengths: dict, noise: float):\n",
    "        super().__init__(seed, ISI_interval, ITI_interval, gamma, num_CS, num_activation_patterns,\n",
    "                         activation_patterns_prob, num_distractors, activation_lengths, noise)\n",
    "        self.ISI_interval = (activation_lengths[\"CS\"], activation_lengths[\"CS\"])\n",
    "\n",
    "\n",
    "def compute_return_error(cumulants, predictions, gamma):\n",
    "    num_time_steps = len(cumulants)\n",
    "    returns = np.zeros(num_time_steps)\n",
    "    returns[-1] = cumulants[-1]\n",
    "    for t in range(num_time_steps - 2, -1, -1):\n",
    "        returns[t] = gamma * returns[t + 1] + cumulants[t]\n",
    "    return_error = (predictions - returns) ** 2\n",
    "    MSRE = return_error.mean()\n",
    "    return MSRE, return_error, returns\n",
    "\n",
    "\n",
    "def produce_activation_patterns(rand_num_generator, num_CS, num_activation_patterns):\n",
    "    activated_indices = list(itertools.combinations(np.arange(num_CS), int(num_CS / 2)))\n",
    "    selected_indices = rand_num_generator.choice(np.arange(len(activated_indices)), size=num_activation_patterns,\n",
    "                                                 replace=False)\n",
    "    activation_patterns = np.zeros((num_activation_patterns, num_CS))\n",
    "    for i in range(num_activation_patterns):\n",
    "        activation_patterns[i, activated_indices[selected_indices[i]]] = 1.0\n",
    "    return activation_patterns\n",
    "\n",
    "\n",
    "def binary_match(x, patterns):\n",
    "    if sum(x) == 0:\n",
    "        ones_match = np.ones(patterns.shape[0])\n",
    "    else:\n",
    "        ones_match = np.floor(np.dot(patterns, x) / sum(x))\n",
    "    if sum(1 - x) == 0:\n",
    "        zeros_match = np.ones(patterns.shape[0])\n",
    "    else:\n",
    "        zeros_match = np.floor(np.dot(1 - patterns, 1 - x) / sum(1 - x))\n",
    "    return ones_match * zeros_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
